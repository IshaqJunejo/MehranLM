# Pipeline

## Overview

This directory contains pipeline scripts for processing the corpus, including cleaning and deduplication, exploratory data analysis (EDA), and tokenization.
Each subdirectory contains specific scripts focused on a particular aspect of corpus processing.

## Directory Structure

- `cleaning/`
  - `corpus_cleaning.py`, cleans the corpus by removing unwanted symbols, characters, and diacritics; it also performs deduplication on each file.
  - `parsing_sindhi_legal.py`, parses the `Sindhi_Legal_Dataset.csv` file and writes the questions and answers on a text file.
- `EDA/`
  - `unique_char.py`, a script to identify unique characters in the corpus.
  - `unique_chars.txt`, contains all the unique characters in our corpus; generated by `unique_char.py`.
  - `char_frequency.py`, calculates the frequency of each character in our entire corpus and writes that information to a file.
  - `char_frequency.txt`, contains each character, its name, and how many times it appears in our corpus; generated by `char_frequency.py`.
  - `char_entropy.py`, shows the *character-level entropy* for each file in the corpus, **Raw** and **Cleaned** both.
  - `sentence_length.py`, splits each file in our corpus on punctuations and newlines, tokenizes the sentences, and shows the stats for **Total Tokens**, **Mean Sentence Length**, **Token Lengths in Percentiles**, for each file in the corpus.
  - `__init__.py`, makes the python scritps in the directory usable outside the directory, where it can be imported as a module.
- `QA/`,
  - `analyze_long_sentence.py`, reads the `sindhi_wiki_artciles_cleaned.txt` file, splits into setences, tokens the sentences, and randomly samples 25 of the sentences with token-length of 75+, and writes them to a file, so the semantics of abnormally long sentence be manually analyzed.
  - `sentence_longer_than_75_tokens.txt`, stores 25 randomly sampled sentences of abnormal token-length; generated by `analyze_long_sentences.py`.
- `tokenization/`
  - `tokenizer.py`, used for training the tokenizer, and also for **encoding** and **decoding** *text to tokens* and *tokens to text*; uses the `Tokenizers` library.
  - `tokenizer.json`, used as the *token dictionary* of our tokenizer; generated by `tokenizer.py`.
  - `__init__.py`, makes the python scritps in the directory usable outside the directory, where it can be imported as a module.
